{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Solving Grid World with Value Propagation Networks and Actor-Critic methods\n",
    "This notebook is intended to recreate the main results of this project, namely a comparison between three different models\n",
    "* A random agent (baseline)\n",
    "* An Actor-Critic agent\n",
    "* A VPN agent\n",
    "\n",
    "We start out by defining the environment. The environment has been created from scratch and can be inspected in GridWorld.py.\n",
    "The environment has multiple difficulties for agents. We will be using level 4, which entails random start position, random goal position and random walls, which means the agents have to learn how to navigate random mazes.\n",
    "For simplicity, we will be working on a 5x5 grid, which still makes this a non-trivial task."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from GridWorld import GridWorld\n",
    "# Hyperparameters\n",
    "#GIVE_UP = 15\n",
    "#N_EPISODES = 10_000\n",
    "LEVEL = 4\n",
    "MAP_SIZE = 5\n",
    "MAP = [MAP_SIZE] * 4\n",
    "WALL_PCT = 0.32\n",
    "#TEST_COUNT = 200\n",
    "#LOG_INTERVAL = 400\n",
    "#DO_INTERMEDIATE_TESTS = True\n",
    "\n",
    "#TEST_SIZE = 100\n",
    "#LEARNING_RATE = 0.001\n",
    "#GAMMA = 0.99\n",
    "SEED = 543\n",
    "#REGULARIZATION_SCALAR = 0.002\n",
    "#FPS = 0\n",
    "\n",
    "env = GridWorld(map=MAP, seed=SEED, non_diag=False, rewards=(0.0, 1.0), wall_pct=WALL_PCT)\n",
    "\n",
    "env.set_level(LEVEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Importing for the neural networks:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from math import prod"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we define our VPN class."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "K = 10 # Iterations in planning module\n",
    "\n",
    "class VPN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VPN, self).__init__()\n",
    "        hidden_units = 32\n",
    "        hidden_units2 = 64\n",
    "        hidden_units_policy1 = 32\n",
    "\n",
    "        self.n_observation1 = env.observation_space.shape[1]\n",
    "        self.n_observation2 = env.observation_space.shape[2]\n",
    "        n_state_dims = self.n_observation1*self.n_observation2\n",
    "        n_actions = len(env.DIRS)\n",
    "\n",
    "        #input should contain\n",
    "        self.affine1 = nn.Linear(prod(env.observation_space.shape), hidden_units)\n",
    "        self.affine2 = nn.Linear(hidden_units, hidden_units2)\n",
    "\n",
    "        # r_out's head\n",
    "        self.r_out = nn.Linear(hidden_units2, n_state_dims)\n",
    "\n",
    "        # r_in's head\n",
    "        self.r_in = nn.Linear(hidden_units2, n_state_dims)\n",
    "\n",
    "        # transition probability head\n",
    "        self.p = nn.Linear(hidden_units2, n_state_dims)\n",
    "\n",
    "        #policy network stuff\n",
    "        self.policyNetwork1 = nn.Linear(n_state_dims*4, hidden_units_policy1) #3 because we don't use the transition probabilities\n",
    "        self.policyHead = nn.Linear(hidden_units_policy1, n_actions)\n",
    "\n",
    "\n",
    "        # action & reward buffer\n",
    "        self.saved_actions = []\n",
    "        self.saved_probabilities_of_actions = []\n",
    "        self.rewards = []\n",
    "        self.shape_of_board = (env.observation_space.shape[1], env.observation_space.shape[2])\n",
    "        self.v_current = torch.zeros(self.shape_of_board)\n",
    "        self.v_next = torch.zeros(self.shape_of_board)\n",
    "        #self.v = torch.zeros(self.shape_of_board)\n",
    "        #self.values = np.zeros(())\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Assumes x to be a (3, i, j) shape\n",
    "        \"\"\"\n",
    "        current_position = (x[1]==1).nonzero()\n",
    "        x = x.flatten()\n",
    "\n",
    "        x = torch.from_numpy(x).float()\n",
    "        state = x\n",
    "        x = f.relu(self.affine1(x))\n",
    "        x = f.relu(self.affine2(x))\n",
    "\n",
    "        r_out = torch.sigmoid(self.r_out(x))\n",
    "\n",
    "        r_out = torch.reshape(r_out, self.shape_of_board)\n",
    "\n",
    "\n",
    "        r_in = torch.sigmoid(self.r_in(x))\n",
    "        r_in = torch.reshape(r_in, self.shape_of_board)\n",
    "\n",
    "\n",
    "        p = torch.sigmoid(self.p(x))\n",
    "        p = torch.reshape(p, self.shape_of_board)\n",
    "\n",
    "        #value iteration\n",
    "\n",
    "        #For all neighborhoods for all states, we define the value of the state, as the value of having taking the best action\n",
    "        #We do this K times\n",
    "        #Notably, because we do this for all states we can get information from states infinitely long away!\n",
    "\n",
    "        #self.v = torch.zeros(self.shape_of_board)\n",
    "\n",
    "        # Padding all grids with zeros\n",
    "        v = f.pad(torch.zeros(self.shape_of_board), (1,1,1,1))\n",
    "        p     = f.pad(p, (1,1,1,1))\n",
    "        r_in  = f.pad(r_in, (1,1,1,1))\n",
    "        r_out = f.pad(r_out, (1,1,1,1))\n",
    "\n",
    "        for k in range(K):\n",
    "            i = 0\n",
    "            helper = torch.zeros((8, self.n_observation1, self.n_observation2)) # 8 directions\n",
    "            # helper = torch.zeros((9, self.n_observation1, self.n_observation2)) # Stay + 8 directions\n",
    "            for i_dot, j_dot in env.DIRS:  # For all directions (env uses 0 dim as x and 1 dim as y)\n",
    "\n",
    "                #logic of indexing: Applied the same for v, p, r_in, r_out\n",
    "                #we take the padded x, index only the \"inner\" v by 1:1+shape_of_board, then\n",
    "                #move the \"square\" we index in the direction of i_dot, j_dot\n",
    "                xs, xe = j_dot+1, 1+j_dot+self.shape_of_board[0]  # +1 because of padding\n",
    "                ys, ye = i_dot+1, 1+i_dot+self.shape_of_board[1]\n",
    "                helper[i] = v[xs:xe,     ys:ye] *  \\\n",
    "                            p[xs:xe,     ys:ye] +  \\\n",
    "                            r_in[xs:xe,  ys:ye] - \\\n",
    "                            r_out[xs:xe, ys:ye]\n",
    "                i +=1\n",
    "            # just the previous v without the padding\n",
    "\n",
    "            v = helper.max(dim=0)[0]  # max over the neighborhood\n",
    "            if k < K-1:  # don't pad if it's the last round\n",
    "                v = f.pad(v, (1,1,1,1))\n",
    "\n",
    "        #policy\n",
    "        input_to_policy = torch.cat((v.flatten(), state), 0)\n",
    "        action_logits = f.relu(self.policyNetwork1(input_to_policy))\n",
    "        action_logits = self.policyHead(action_logits)\n",
    "        action_prob = f.softmax(action_logits, dim=-1)\n",
    "\n",
    "        #value at current state\n",
    "\n",
    "        state_value = v[current_position]\n",
    "\n",
    "        return action_prob, state_value\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}